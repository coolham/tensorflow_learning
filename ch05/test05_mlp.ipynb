{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:1.13.1\n",
      "NumPy:1.16.3\n",
      "['c:\\\\python36\\\\python36.zip', 'c:\\\\python36\\\\DLLs', 'c:\\\\python36\\\\lib', 'c:\\\\python36', '', 'c:\\\\python36\\\\lib\\\\site-packages', 'c:\\\\python36\\\\lib\\\\site-packages\\\\pip-9.0.1-py3.6.egg', 'c:\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\WJ\\\\.ipython', 'D:\\\\Work\\\\tensorflow_test']\n",
      "WARNING:tensorflow:From <ipython-input-1-e7b2d3f52f35>:26: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./datasets\\mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./datasets\\mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./datasets\\mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets\\mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-e7b2d3f52f35>:108: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch 0000 loss = 7.879391\n",
      "epoch 0001 loss = 4.361162\n",
      "epoch 0002 loss = 3.112700\n",
      "epoch 0003 loss = 2.477692\n",
      "epoch 0004 loss = 2.098666\n",
      "epoch 0005 loss = 1.846451\n",
      "epoch 0006 loss = 1.666961\n",
      "epoch 0007 loss = 1.532406\n",
      "epoch 0008 loss = 1.427664\n",
      "epoch 0009 loss = 1.343934\n",
      "epoch 0010 loss = 1.275065\n",
      "epoch 0011 loss = 1.217496\n",
      "epoch 0012 loss = 1.168679\n",
      "epoch 0013 loss = 1.126378\n",
      "epoch 0014 loss = 1.089537\n",
      "epoch 0015 loss = 1.056890\n",
      "epoch 0016 loss = 1.027783\n",
      "epoch 0017 loss = 1.001721\n",
      "epoch 0018 loss = 0.977934\n",
      "epoch 0019 loss = 0.956372\n",
      "epoch 0020 loss = 0.936541\n",
      "epoch 0021 loss = 0.918399\n",
      "epoch 0022 loss = 0.901317\n",
      "epoch 0023 loss = 0.885626\n",
      "epoch 0024 loss = 0.871032\n",
      "epoch 0025 loss = 0.857294\n",
      "epoch 0026 loss = 0.844343\n",
      "epoch 0027 loss = 0.832208\n",
      "epoch 0028 loss = 0.820894\n",
      "epoch 0029 loss = 0.809966\n",
      "epoch 0030 loss = 0.799659\n",
      "epoch 0031 loss = 0.789896\n",
      "epoch 0032 loss = 0.780631\n",
      "epoch 0033 loss = 0.771768\n",
      "epoch 0034 loss = 0.763230\n",
      "epoch 0035 loss = 0.755106\n",
      "epoch 0036 loss = 0.747353\n",
      "epoch 0037 loss = 0.739934\n",
      "epoch 0038 loss = 0.732763\n",
      "epoch 0039 loss = 0.725770\n",
      "epoch 0040 loss = 0.719247\n",
      "epoch 0041 loss = 0.712747\n",
      "epoch 0042 loss = 0.706522\n",
      "epoch 0043 loss = 0.700592\n",
      "epoch 0044 loss = 0.694874\n",
      "epoch 0045 loss = 0.689287\n",
      "epoch 0046 loss = 0.683892\n",
      "epoch 0047 loss = 0.678629\n",
      "epoch 0048 loss = 0.673638\n",
      "epoch 0049 loss = 0.668665\n",
      "accuracy = 0.86180001\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow:{}'.format(tf.__version__))\n",
    "import numpy as np\n",
    "print('NumPy:{}'.format(np.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "current_path = os.getcwd()\n",
    "base_dir = os.path.dirname(current_path)\n",
    "\n",
    "if not base_dir in sys.path:\n",
    "    sys.path.append(base_dir)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "\n",
    "mnist_home = os.path.join(datasetslib.datasets_root, 'mnist')\n",
    "mnist = input_data.read_data_sets(mnist_home, one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "num_outputs = 10\n",
    "num_inputs = 784\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 多层感知机\n",
    "def mlp(x, num_inputs, num_outputs, num_layers, num_neurons):\n",
    "    w = []\n",
    "    b = []\n",
    "    for i in range(num_layers):\n",
    "        # 权重\n",
    "        w.append(tf.Variable(tf.random_normal(\n",
    "            [num_inputs if i==0 else num_neurons[i-1], num_neurons[i]]), \n",
    "                             name='w_{0:04d}'.format(i)\n",
    "                            ))\n",
    "\n",
    "        # 偏差\n",
    "        b.append(tf.Variable(tf.random_normal([num_neurons[i]]), name='b_{0:04d}'.format(i)\n",
    "                            ))\n",
    "        \n",
    "    w.append(tf.Variable(tf.random_normal(\n",
    "        [num_neurons[num_layers-1] if num_layers > 0 else num_inputs, num_outputs]),\n",
    "                        name='w_out'))\n",
    "    b.append(tf.Variable(tf.random_normal([num_outputs]), name='b_out'))\n",
    "    \n",
    "    # x是输入层\n",
    "    layer = x\n",
    "    # 添加隐藏层\n",
    "    for i in range(num_layers):\n",
    "        layer = tf.nn.relu(tf.matmul(layer, w[i]) + b[i])\n",
    "    # 添加输出层\n",
    "    layer = tf.matmul(layer, w[num_layers]) + b[num_layers]\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def mnist_batch_func(batch_size=100):\n",
    "    x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "    return [x_batch, y_batch]\n",
    "\n",
    "\n",
    "def tensorflow_classification(n_epochs, n_batches, batch_size, batch_func, model, optimizer, loss, accuracy_function, X_test, Y_test):\n",
    "    with tf.Session() as tfs:\n",
    "        tfs.run(tf.global_variables_initializer())\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for batch in range(n_batches):\n",
    "                X_batch, Y_batch = batch_func(batch_size)\n",
    "                feed_dict = {x: X_batch, y: Y_batch}\n",
    "                _, batch_loss = tfs.run([optimizer, loss], feed_dict)\n",
    "                epoch_loss += batch_loss\n",
    "            average_loss = epoch_loss / n_batches\n",
    "            print('epoch {0:04d} loss = {1:.6f}'.format(epoch, average_loss))\n",
    "        feed_dict = {x: X_test, y: Y_test}\n",
    "        accuracy_score = tfs.run(accuracy_function, feed_dict=feed_dict)\n",
    "        print('accuracy = {0:.8f}'.format(accuracy_score))\n",
    "\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "    \n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, num_inputs], name='x')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, num_outputs], name='y')\n",
    "\n",
    "num_layers = 0\n",
    "num_neurons = []\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                         n_batches=n_batches,\n",
    "                         batch_size=batch_size,\n",
    "                         batch_func=mnist_batch_func,\n",
    "                         model=model,\n",
    "                         optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         accuracy_function=accuracy_function,\n",
    "                         X_test=mnist.test.images,\n",
    "                         Y_test=mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0000 loss = 3.411459\n",
      "epoch 0001 loss = 2.210572\n",
      "epoch 0002 loss = 2.022875\n",
      "epoch 0003 loss = 1.890177\n",
      "epoch 0004 loss = 1.779281\n",
      "epoch 0005 loss = 1.688049\n",
      "epoch 0006 loss = 1.613588\n",
      "epoch 0007 loss = 1.551283\n",
      "epoch 0008 loss = 1.498727\n",
      "epoch 0009 loss = 1.453876\n",
      "epoch 0010 loss = 1.414699\n",
      "epoch 0011 loss = 1.380326\n",
      "epoch 0012 loss = 1.349421\n",
      "epoch 0013 loss = 1.320288\n",
      "epoch 0014 loss = 1.294798\n",
      "epoch 0015 loss = 1.270293\n",
      "epoch 0016 loss = 1.247314\n",
      "epoch 0017 loss = 1.225995\n",
      "epoch 0018 loss = 1.204541\n",
      "epoch 0019 loss = 1.184105\n",
      "epoch 0020 loss = 1.163835\n",
      "epoch 0021 loss = 1.143601\n",
      "epoch 0022 loss = 1.123723\n",
      "epoch 0023 loss = 1.104597\n",
      "epoch 0024 loss = 1.085514\n",
      "epoch 0025 loss = 1.066914\n",
      "epoch 0026 loss = 1.048135\n",
      "epoch 0027 loss = 1.031393\n",
      "epoch 0028 loss = 1.013850\n",
      "epoch 0029 loss = 0.996959\n",
      "epoch 0030 loss = 0.980240\n",
      "epoch 0031 loss = 0.964548\n",
      "epoch 0032 loss = 0.948476\n",
      "epoch 0033 loss = 0.933188\n",
      "epoch 0034 loss = 0.918711\n",
      "epoch 0035 loss = 0.904572\n",
      "epoch 0036 loss = 0.890275\n",
      "epoch 0037 loss = 0.877757\n",
      "epoch 0038 loss = 0.864413\n",
      "epoch 0039 loss = 0.852854\n",
      "epoch 0040 loss = 0.842261\n",
      "epoch 0041 loss = 0.830917\n",
      "epoch 0042 loss = 0.821237\n",
      "epoch 0043 loss = 0.811152\n",
      "epoch 0044 loss = 0.802097\n",
      "epoch 0045 loss = 0.793242\n",
      "epoch 0046 loss = 0.783747\n",
      "epoch 0047 loss = 0.775722\n",
      "epoch 0048 loss = 0.766940\n",
      "epoch 0049 loss = 0.759670\n",
      "accuracy = 0.76700002\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(8)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                         n_batches=n_batches,\n",
    "                         batch_size=batch_size,\n",
    "                         batch_func=mnist_batch_func,\n",
    "                         model=model,\n",
    "                         optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         accuracy_function=accuracy_function,\n",
    "                         X_test=mnist.test.images,\n",
    "                         Y_test=mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0000 loss = 58.404377\n",
      "epoch 0001 loss = 12.860040\n",
      "epoch 0002 loss = 7.908385\n",
      "epoch 0003 loss = 5.658149\n",
      "epoch 0004 loss = 4.232787\n",
      "epoch 0005 loss = 3.346118\n",
      "epoch 0006 loss = 2.709890\n",
      "epoch 0007 loss = 2.239875\n",
      "epoch 0008 loss = 1.866765\n",
      "epoch 0009 loss = 1.617455\n",
      "epoch 0010 loss = 1.375186\n",
      "epoch 0011 loss = 1.191594\n",
      "epoch 0012 loss = 1.034016\n",
      "epoch 0013 loss = 0.901348\n",
      "epoch 0014 loss = 0.793413\n",
      "epoch 0015 loss = 0.714670\n",
      "epoch 0016 loss = 0.634422\n",
      "epoch 0017 loss = 0.562480\n",
      "epoch 0018 loss = 0.491850\n",
      "epoch 0019 loss = 0.436744\n",
      "epoch 0020 loss = 0.384074\n",
      "epoch 0021 loss = 0.347512\n",
      "epoch 0022 loss = 0.309014\n",
      "epoch 0023 loss = 0.276722\n",
      "epoch 0024 loss = 0.248843\n",
      "epoch 0025 loss = 0.226315\n",
      "epoch 0026 loss = 0.205495\n",
      "epoch 0027 loss = 0.183316\n",
      "epoch 0028 loss = 0.161380\n",
      "epoch 0029 loss = 0.148160\n",
      "epoch 0030 loss = 0.134875\n",
      "epoch 0031 loss = 0.121845\n",
      "epoch 0032 loss = 0.105294\n",
      "epoch 0033 loss = 0.097690\n",
      "epoch 0034 loss = 0.087702\n",
      "epoch 0035 loss = 0.081179\n",
      "epoch 0036 loss = 0.071706\n",
      "epoch 0037 loss = 0.062995\n",
      "epoch 0038 loss = 0.062080\n",
      "epoch 0039 loss = 0.052858\n",
      "epoch 0040 loss = 0.048781\n",
      "epoch 0041 loss = 0.044769\n",
      "epoch 0042 loss = 0.041858\n",
      "epoch 0043 loss = 0.036618\n",
      "epoch 0044 loss = 0.033719\n",
      "epoch 0045 loss = 0.030167\n",
      "epoch 0046 loss = 0.027761\n",
      "epoch 0047 loss = 0.025121\n",
      "epoch 0048 loss = 0.020456\n",
      "epoch 0049 loss = 0.020826\n",
      "accuracy = 0.93430001\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(256)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                         n_batches=n_batches,\n",
    "                         batch_size=batch_size,\n",
    "                         batch_func=mnist_batch_func,\n",
    "                         model=model,\n",
    "                         optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         accuracy_function=accuracy_function,\n",
    "                         X_test=mnist.test.images,\n",
    "                         Y_test=mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0000 loss = 2.746121\n",
      "epoch 0001 loss = 2.223680\n",
      "epoch 0002 loss = 2.172618\n",
      "epoch 0003 loss = 2.139252\n",
      "epoch 0004 loss = 2.105271\n",
      "epoch 0005 loss = 2.071610\n",
      "epoch 0006 loss = 2.052745\n",
      "epoch 0007 loss = 2.027435\n",
      "epoch 0008 loss = 2.005989\n",
      "epoch 0009 loss = 1.989630\n",
      "epoch 0010 loss = 1.968230\n",
      "epoch 0011 loss = 1.950378\n",
      "epoch 0012 loss = 1.933832\n",
      "epoch 0013 loss = 1.918492\n",
      "epoch 0014 loss = 1.897512\n",
      "epoch 0015 loss = 1.890226\n",
      "epoch 0016 loss = 1.872777\n",
      "epoch 0017 loss = 1.860911\n",
      "epoch 0018 loss = 1.846063\n",
      "epoch 0019 loss = 1.837533\n",
      "epoch 0020 loss = 1.822538\n",
      "epoch 0021 loss = 1.815060\n",
      "epoch 0022 loss = 1.806092\n",
      "epoch 0023 loss = 1.794899\n",
      "epoch 0024 loss = 1.785806\n",
      "epoch 0025 loss = 1.778228\n",
      "epoch 0026 loss = 1.768430\n",
      "epoch 0027 loss = 1.761350\n",
      "epoch 0028 loss = 1.751501\n",
      "epoch 0029 loss = 1.746616\n",
      "epoch 0030 loss = 1.741719\n",
      "epoch 0031 loss = 1.731736\n",
      "epoch 0032 loss = 1.726825\n",
      "epoch 0033 loss = 1.718465\n",
      "epoch 0034 loss = 1.720384\n",
      "epoch 0035 loss = 1.703633\n",
      "epoch 0036 loss = 1.701392\n",
      "epoch 0037 loss = 1.696412\n",
      "epoch 0038 loss = 1.693189\n",
      "epoch 0039 loss = 1.686056\n",
      "epoch 0040 loss = 1.679017\n",
      "epoch 0041 loss = 1.678203\n",
      "epoch 0042 loss = 1.669328\n",
      "epoch 0043 loss = 1.666222\n",
      "epoch 0044 loss = 1.660704\n",
      "epoch 0045 loss = 1.654639\n",
      "epoch 0046 loss = 1.651179\n",
      "epoch 0047 loss = 1.645521\n",
      "epoch 0048 loss = 1.637165\n",
      "epoch 0049 loss = 1.635324\n",
      "accuracy = 0.34970000\n"
     ]
    }
   ],
   "source": [
    "num_layers = 3\n",
    "num_neurons = []\n",
    "for i in range(num_layers):\n",
    "    num_neurons.append(8)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "n_batches = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "model = mlp(x=x,\n",
    "            num_inputs=num_inputs,\n",
    "            num_outputs=num_outputs,\n",
    "            num_layers=num_layers,\n",
    "            num_neurons=num_neurons)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction_check = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "accuracy_function = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "tensorflow_classification(n_epochs=n_epochs,\n",
    "                         n_batches=n_batches,\n",
    "                         batch_size=batch_size,\n",
    "                         batch_func=mnist_batch_func,\n",
    "                         model=model,\n",
    "                         optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         accuracy_function=accuracy_function,\n",
    "                         X_test=mnist.test.images,\n",
    "                         Y_test=mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}