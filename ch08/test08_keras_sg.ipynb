{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow:1.13.1\n",
      "NumPy:1.16.3\n",
      "Pandas:0.24.2\n",
      "['c:\\\\python36\\\\python36.zip', 'c:\\\\python36\\\\DLLs', 'c:\\\\python36\\\\lib', 'c:\\\\python36', '', 'c:\\\\python36\\\\lib\\\\site-packages', 'c:\\\\python36\\\\lib\\\\site-packages\\\\pip-9.0.1-py3.6.egg', 'c:\\\\python36\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\WJ\\\\.ipython', 'D:\\\\Work\\\\tensorflow_test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: ./datasets\\ptb-simple\\simple-examples.tgz\n",
      "valid:  [63 60 46  3 33 68 22 26]\n",
      "The skip-gram pairs: target, context\n",
      "['691 pacific', '9508 trademark'] : 0\n",
      "['5684 guests', '4261 discounts'] : 0\n",
      "['9058 insider-trading', '2735 forms'] : 0\n",
      "['349 without', '314 until'] : 1\n",
      "['3163 controlling', '866 acquire'] : 1\n",
      "['4779 reopen', '779 failed'] : 0\n",
      "['4086 buick', '3658 whitbread'] : 0\n",
      "['716 chemical', '88 such'] : 1\n",
      "['211 among', '1233 death'] : 1\n",
      "['6097 lag', '1 <unk>'] : 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow:{}'.format(tf.__version__))\n",
    "import numpy as np\n",
    "print('NumPy:{}'.format(np.__version__))\n",
    "import pandas as pd\n",
    "print(\"Pandas:{}\".format(pd.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_path = os.getcwd()\n",
    "base_dir = os.path.dirname(current_path)\n",
    "\n",
    "if not base_dir in sys.path:\n",
    "    sys.path.append(base_dir)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import datasetslib\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import skipgrams \n",
    "from keras.layers import Input, Dense, Reshape, Dot, merge\n",
    "import keras\n",
    "\n",
    "from datasetslib.ptb import PTBSimple\n",
    "from datasetslib import util as dsu\n",
    "from datasetslib import nputil\n",
    "\n",
    "ptb = PTBSimple()\n",
    "# 加载数据，将单词转换为ids，将文件转换为ids列表\n",
    "ptb.load_data()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "keras.backend.clear_session()\n",
    "\n",
    "valid_size = 8\n",
    "x_valid = np.random.choice(valid_size * 10, valid_size, replace=False)\n",
    "print('valid: ', x_valid)\n",
    "\n",
    "batch_size = 1024\n",
    "embedding_size = 512\n",
    "n_negative_samples = 64\n",
    "ptb.skip_window = 2\n",
    "\n",
    "sample_table = sequence.make_sampling_table(ptb.vocab_len)\n",
    "pairs, labels = sequence.skipgrams(ptb.part['train'], ptb.vocab_len, window_size=ptb.skip_window, sampling_table=sample_table)\n",
    "\n",
    "print('The skip-gram pairs: target, context')\n",
    "for i in range(5 * ptb.skip_window):\n",
    "    print(['{} {}'.format(id, ptb.id2word[id]) for id in pairs[i]], ':', labels[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-868dc0d1d101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 将目标和上下文单词转换为二维数组\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnputil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pairs' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 将目标和上下文单词转换为二维数组\n",
    "x, y = zip(*pairs)\n",
    "x = np.array(x, dtype=np.int32)\n",
    "x = nputil.to2d(x, unit_axis=1)\n",
    "\n",
    "y = np.array(y, dtype=np.int32)\n",
    "y = nputil.to2d(y, unit_axis=1)\n",
    "\n",
    "labels = np.array(labels, dtype=np.int32)\n",
    "labels = nputil.to2d(labels, unit_axis=1)\n",
    "\n",
    "# 构建目标词模型\n",
    "target_in = Input(shape=(1,), name='target_in')\n",
    "target = Embedding(ptb.vocab_len, embedding_size, input_length=1,\n",
    "                  name='target_em')(target_in)\n",
    "target = Reshape((embedding_size, 1), name='target_re')(target)\n",
    "\n",
    "# 构建上下文词模型\n",
    "context_in = Input((1,), name='context_in')\n",
    "context = Embedding(ptb.vocab_len, embedding_size, input_length=1,\n",
    "                   name='context_em')(context_in)\n",
    "context = Reshape((embedding_size, 1), name='context_re')(context)\n",
    "\n",
    "# 将两个模型内积以检查相似性并添加sigmoid层\n",
    "output = Dot(axes=1, name='output_dot')([target, context])\n",
    "output = Reshape((1,), name='output_re')(output)\n",
    "output = Dense(1, activation='sigmoid', name='output_sig')(output)\n",
    "\n",
    "# 创建用于查找词向量的功能性模型l\n",
    "model = Model(inputs=[target_in, context_in], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# 合并模型并创建模型以检查余弦相似性\n",
    "similarity = Dot(axes=0, normalize=True, name='sim_dot')([target, context])\n",
    "similarity_model = Model(inputs=[target_in, context_in], outputs=similarity)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 1024\n",
    "\n",
    "model.fit([x, y], labels, batch_size=batch_size, epochs=n_epochs)\n",
    "\n",
    "top_k = 5\n",
    "batch_size = 1024\n",
    "y_val = np.arange(ptb.vocab_len, dtype=np.int32)\n",
    "y_val = nputil.to2d(y_val, unit_axis=1)\n",
    "\n",
    "for i in range(valid_size):\n",
    "    x_val = np.full(shape=(ptb.vocab_len, 1), fill_value=x_valid[1], dtype=np.int32)\n",
    "    similarity_scores = similarity_model.predict([x_val, y_val])\n",
    "    similarity_scores = similarity_scores.flatten()\n",
    "    similar_words = (-similarity_scores).argsort()[1:top_k+1]\n",
    "    similar_str = 'Similar to {0:}:'.format(ptb.id2word[x_valid[i]])\n",
    "    for k in range(top_k):\n",
    "        similar_str = '{0:} {1:}'.format(similar_str, ptb.id2word[similar_words[k]])\n",
    "    print(similar_str)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
